"""
Hybrid RAG è©•ä¼°å„€è¡¨æ¿

Streamlit å‰ç«¯æ‡‰ç”¨ç¨‹å¼

ä½¿ç”¨æ–¹å¼ï¼š
    uv run streamlit run app.py
"""

import json
import time
from pathlib import Path
from datetime import datetime

import streamlit as st
import pandas as pd

from services import RAGService
from core.config import settings
from models.document import QueryModel


# é é¢é…ç½®
st.set_page_config(
    page_title="Hybrid RAG è©•ä¼°å„€è¡¨æ¿",
    page_icon="ğŸ”",
    layout="wide",
)

DATA_DIR = Path("data")
MODES = ["hybrid", "vector", "keyword"]


# ===================== è³‡æ–™ç®¡ç† =====================

def get_result_path(mode: str) -> Path:
    return DATA_DIR / f"rag_results_{mode}.json"


def get_metrics_path(mode: str) -> Path:
    return DATA_DIR / f"evaluation_metrics_{mode}.json"


def get_answer_eval_path(mode: str) -> Path:
    return DATA_DIR / f"answer_evaluation_{mode}.json"


def load_existing_results() -> dict:
    """è¼‰å…¥å·²å­˜åœ¨çš„çµæœæª”æ¡ˆï¼ˆåŒ…å« LLM è©•ä¼°ï¼‰"""
    results = {}
    
    for mode in MODES:
        path = get_result_path(mode)
        if path.exists():
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            mode_results = data.get("results", data)
            
            # å˜—è©¦è¼‰å…¥ LLM è©•ä¼°çµæœä¸¦åˆä½µ
            eval_path = get_answer_eval_path(mode)
            if eval_path.exists():
                with open(eval_path, "r", encoding="utf-8") as f:
                    eval_data = json.load(f)
                eval_results = eval_data.get("results", [])
                
                # å»ºç«‹ question_id -> eval_result çš„æ˜ å°„
                eval_map = {e["question_id"]: e for e in eval_results}
                
                # åˆä½µ LLM è©•ä¼°è³‡æ–™
                for r in mode_results:
                    q_id = r.get("question_id")
                    if q_id and q_id in eval_map:
                        r["llm_judgment"] = eval_map[q_id].get("llm_judgment")
                        r["is_pass"] = eval_map[q_id].get("is_pass", False)
            
            results[mode] = mode_results
    
    return results


def save_results(mode: str, results: list[dict], metadata: dict = None):
    """å„²å­˜çµæœåˆ°æª”æ¡ˆ"""
    output_data = {
        "metadata": metadata or {},
        "results": results,
    }
    path = get_result_path(mode)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)


def load_queries() -> list[QueryModel]:
    """è¼‰å…¥æŸ¥è©¢"""
    with open(settings.queries_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return [QueryModel(**q) for q in data]


# ===================== è©•ä¼°åŸ·è¡Œ =====================

def run_evaluation(queries: list[QueryModel], mode: str, top_k: int) -> list[dict]:
    """åŸ·è¡Œè©•ä¼°"""
    if "rag_service" not in st.session_state:
        st.session_state.rag_service = RAGService()
    
    rag = st.session_state.rag_service
    rag.initialize(mode=mode)
    
    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_time = 0.0
    
    for i, query in enumerate(queries):
        status_text.text(f"è™•ç†ä¸­: {i+1}/{len(queries)}")
        
        start_time = time.perf_counter()
        response = rag.answer(query.question, top_k=top_k, mode=mode)
        elapsed_time = time.perf_counter() - start_time
        total_time += elapsed_time
        
        gold_ids = set(query.gold_doc_ids)
        retrieved_ids = set(response.retrieved_doc_ids)
        hit_ids = gold_ids.intersection(retrieved_ids)
        
        result = {
            "question_id": query.question_id,
            "question": query.question,
            "question_type": query.question_type,
            "source_dataset": query.source_dataset,
            "gold_answer": query.gold_answer,
            "gold_doc_ids": query.gold_doc_ids,
            "generated_answer": response.answer,
            "retrieved_doc_ids": response.retrieved_doc_ids,
            "contexts": [
                {
                    "doc_id": ctx.doc_id,
                    "score": ctx.score,
                    "content": ctx.content,
                    "original_source": ctx.original_source,
                }
                for ctx in response.contexts
            ],
            "hit_count": len(hit_ids),
            "gold_count": len(gold_ids),
            "partial_hit": f"{len(hit_ids)}/{len(gold_ids)}",
            "is_hit": len(hit_ids) > 0,
            "response_time_ms": round(elapsed_time * 1000, 2),
        }
        results.append(result)
        progress_bar.progress((i + 1) / len(queries))
    
    status_text.text("âœ… å®Œæˆ!")
    
    # å„²å­˜çµæœ
    metadata = {
        "mode": mode,
        "top_k": top_k,
        "total_questions": len(results),
        "total_time_seconds": round(total_time, 2),
        "avg_response_time_ms": round((total_time / len(results)) * 1000, 2),
        "timestamp": datetime.now().isoformat(),
    }
    save_results(mode, results, metadata)
    
    return results


# ===================== æŒ‡æ¨™è¨ˆç®— =====================

def calculate_metrics(results: list[dict]) -> dict:
    """è¨ˆç®—æª¢ç´¢èˆ‡ç”ŸæˆæŒ‡æ¨™"""
    total = len(results)
    if total == 0:
        return {}
    
    # é è™•ç†ï¼šè¨ˆç®—ç¼ºå¤±çš„æ¬„ä½
    for r in results:
        gold_ids = set(r.get("gold_doc_ids", []))
        retrieved_ids = set(r.get("retrieved_doc_ids", []))
        hit_ids = gold_ids.intersection(retrieved_ids)
        
        if "hit_count" not in r:
            r["hit_count"] = len(hit_ids)
        if "gold_count" not in r:
            r["gold_count"] = len(gold_ids)
        if "is_hit" not in r:
            r["is_hit"] = len(hit_ids) > 0
    
    # æª¢ç´¢æŒ‡æ¨™
    total_hits = sum(1 for r in results if r.get("is_hit", False))
    total_gold_docs = sum(r.get("gold_count", 0) for r in results)
    total_hit_docs = sum(r.get("hit_count", 0) for r in results)
    avg_time = sum(r.get("response_time_ms", 0) for r in results) / total
    
    # å–®ä¸€ gold doc çš„ hit rate
    single_gold = [r for r in results if r.get("gold_count", 0) == 1]
    single_hits = sum(1 for r in single_gold if r.get("is_hit", False))
    
    # MRRï¼ˆå¹³å‡ RRï¼‰
    def calc_mrr(results):
        total_rr = 0
        for r in results:
            gold_ids = set(r.get("gold_doc_ids", []))
            retrieved_ids = r.get("retrieved_doc_ids", [])
            rr_sum = 0
            for gold_id in gold_ids:
                for rank, doc_id in enumerate(retrieved_ids, start=1):
                    if doc_id == gold_id:
                        rr_sum += 1.0 / rank
                        break
            total_rr += rr_sum / len(gold_ids) if gold_ids else 0
        return total_rr / len(results) if results else 0
    
    # ç”ŸæˆæŒ‡æ¨™ï¼ˆå¦‚æœæœ‰ LLM è©•ä¼°çµæœï¼‰
    has_llm_eval = any("is_pass" in r for r in results)
    if has_llm_eval:
        passed = sum(1 for r in results if r.get("is_pass", False))
        pass_rate = passed / total
    else:
        passed = None
        pass_rate = None
    
    return {
        "total_questions": total,
        "hit_rate": total_hits / total,
        "single_gold_hit_rate": single_hits / len(single_gold) if single_gold else 0,
        "partial_hit_rate": total_hit_docs / total_gold_docs if total_gold_docs > 0 else 0,
        "mrr": calc_mrr(results),
        "avg_response_time_ms": avg_time,
        # ç”ŸæˆæŒ‡æ¨™
        "llm_passed": passed,
        "llm_pass_rate": pass_rate,
    }


# ===================== LLM èªæ„è©•ä¼° =====================

def run_llm_evaluation(results: list[dict], mode: str) -> list[dict]:
    """åŸ·è¡Œ LLM èªæ„è©•ä¼°"""
    from openai import OpenAI
    
    client = OpenAI(api_key=settings.OPENAI_API_KEY)
    
    PROMPT = """è«‹åˆ¤æ–·ã€Œæ¨¡å‹å›ç­”ã€æ˜¯å¦èˆ‡ã€Œæ¨™æº–ç­”æ¡ˆã€èªæ„ä¸€è‡´ã€‚

å•é¡Œï¼š{question}
æ¨™æº–ç­”æ¡ˆï¼š{gold_answer}
æ¨¡å‹å›ç­”ï¼š{model_answer}

åˆ¤æ–·æ¨™æº–ï¼š
- å¦‚æœæ¨¡å‹å›ç­”åŒ…å«æ¨™æº–ç­”æ¡ˆçš„æ ¸å¿ƒè³‡è¨Šï¼Œä¸”æ²’æœ‰æ˜é¡¯éŒ¯èª¤ï¼Œè«‹å›ç­” "Pass"
- å¦‚æœæ¨¡å‹å›ç­”èˆ‡æ¨™æº–ç­”æ¡ˆèªæ„ä¸ä¸€è‡´ã€æœ‰éŒ¯èª¤ã€æˆ–å®Œå…¨ç„¡é—œï¼Œè«‹å›ç­” "Fail"

è«‹åªå›ç­” "Pass" æˆ– "Fail"ã€‚"""
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, r in enumerate(results):
        status_text.text(f"èªæ„è©•ä¼°ä¸­: {i+1}/{len(results)}")
        
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": PROMPT.format(
                    question=r["question"],
                    gold_answer=r.get("gold_answer", ""),
                    model_answer=r.get("generated_answer", ""),
                )}],
                temperature=0,
                max_tokens=10,
            )
            raw = response.choices[0].message.content.strip()
            r["llm_judgment"] = raw
            r["is_pass"] = raw.lower() == "pass"
        except Exception as e:
            r["llm_judgment"] = f"Error: {e}"
            r["is_pass"] = False
        
        progress_bar.progress((i + 1) / len(results))
    
    status_text.text("âœ… èªæ„è©•ä¼°å®Œæˆ!")
    
    # æ›´æ–°å„²å­˜
    save_results(mode, results)
    
    return results


# ===================== UI å…ƒä»¶ =====================

def calculate_grouped_metrics(results: list[dict]) -> dict:
    """è¨ˆç®—åˆ†çµ„æŒ‡æ¨™"""
    from collections import defaultdict
    
    # é è™•ç†
    for r in results:
        gold_ids = set(r.get("gold_doc_ids", []))
        retrieved_ids = set(r.get("retrieved_doc_ids", []))
        hit_ids = gold_ids.intersection(retrieved_ids)
        r["hit_count"] = len(hit_ids)
        r["gold_count"] = len(gold_ids)
        r["is_hit"] = len(hit_ids) > 0
    
    def calc_mrr(subset):
        total_rr = 0
        for r in subset:
            gold_ids = set(r.get("gold_doc_ids", []))
            retrieved_ids = r.get("retrieved_doc_ids", [])
            rr_sum = 0
            for gold_id in gold_ids:
                for rank, doc_id in enumerate(retrieved_ids, start=1):
                    if doc_id == gold_id:
                        rr_sum += 1.0 / rank
                        break
            total_rr += rr_sum / len(gold_ids) if gold_ids else 0
        return total_rr / len(subset) if subset else 0
    
    def calc_group(subset):
        total = len(subset)
        gold_docs = sum(r["gold_count"] for r in subset)
        hit_docs = sum(r["hit_count"] for r in subset)
        single_gold = [r for r in subset if r["gold_count"] == 1]
        single_hits = sum(1 for r in single_gold if r["is_hit"])
        return {
            "total": total,
            "gold_docs": gold_docs,
            "hit_docs": hit_docs,
            "partial_hit_rate": hit_docs / gold_docs if gold_docs > 0 else 0,
            "hit_rate": single_hits / len(single_gold) if single_gold else None,
            "mrr": calc_mrr(subset),
        }
    
    # æŒ‰è³‡æ–™ä¾†æºåˆ†çµ„
    by_source = defaultdict(list)
    for r in results:
        by_source[r.get("source_dataset", "unknown")].append(r)
    
    # æŒ‰å•é¡Œé¡å‹åˆ†çµ„
    by_type = defaultdict(list)
    for r in results:
        by_type[r.get("question_type", "unknown")].append(r)
    
    return {
        "by_source": {k: calc_group(v) for k, v in by_source.items()},
        "by_type": {k: calc_group(v) for k, v in by_type.items()},
        "total": calc_group(results),
    }


def display_metrics_comparison(all_results: dict):
    """é¡¯ç¤ºæŒ‡æ¨™æ¯”è¼ƒï¼ˆä¸‰æ¨¡å¼ä¸¦æ’ï¼‰"""
    if not all_results:
        st.info("å°šç„¡è©•ä¼°çµæœã€‚è«‹åŸ·è¡Œè©•ä¼°æˆ–ç¢ºèª data ç›®éŒ„ä¸­æœ‰çµæœæª”æ¡ˆã€‚")
        return
    
    # é¸æ“‡æ¨¡å¼
    available_modes = [m for m in MODES if m in all_results]
    
    # Tab: æ¯”è¼ƒåœ–è¡¨ vs è©³ç´°å ±å‘Š
    sub_tab1, sub_tab2 = st.tabs(["ğŸ“Š æ¨¡å¼æ¯”è¼ƒ", "ğŸ“‹ è©³ç´°å ±å‘Š"])
    
    with sub_tab1:
        st.subheader("ğŸ“Š ä¸‰æ¨¡å¼æŒ‡æ¨™æ¯”è¼ƒ")
        
        # è¨ˆç®—å„æ¨¡å¼æŒ‡æ¨™
        metrics_list = []
        for mode in MODES:
            if mode in all_results:
                m = calculate_metrics(all_results[mode])
                m["mode"] = mode
                metrics_list.append(m)
        
        if not metrics_list:
            return
        
        df = pd.DataFrame(metrics_list).set_index("mode")
        
        # æŒ‡æ¨™é¸æ“‡
        metric_options = {
            "Hit Rate": "hit_rate",
            "Partial Hit Rate": "partial_hit_rate",
            "MRR": "mrr",
            "Avg Response Time (ms)": "avg_response_time_ms",
        }
        if df["llm_pass_rate"].notna().any():
            metric_options["LLM Pass Rate"] = "llm_pass_rate"
        
        selected_metric = st.selectbox("é¸æ“‡æŒ‡æ¨™", list(metric_options.keys()))
        metric_col = metric_options[selected_metric]
        
        # é•·æ¢åœ–
        chart_data = df[[metric_col]].dropna()
        chart_data.columns = [selected_metric]
        st.bar_chart(chart_data)
        
        # å®Œæ•´è¡¨æ ¼
        st.markdown("### å®Œæ•´æŒ‡æ¨™è¡¨")
        display_df = df[["hit_rate", "partial_hit_rate", "mrr", "avg_response_time_ms"]].copy()
        display_df.columns = ["Hit Rate", "Partial HR", "MRR", "Avg Time (ms)"]
        for col in ["Hit Rate", "Partial HR"]:
            display_df[col] = display_df[col].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "-")
        display_df["MRR"] = display_df["MRR"].apply(lambda x: f"{x:.4f}" if pd.notna(x) else "-")
        display_df["Avg Time (ms)"] = display_df["Avg Time (ms)"].apply(lambda x: f"{x:.0f}" if pd.notna(x) else "-")
        if "llm_pass_rate" in df.columns:
            display_df["LLM Pass Rate"] = df["llm_pass_rate"].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "-")
        st.dataframe(display_df.T, width="stretch")
    
    with sub_tab2:
        selected_mode = st.selectbox("é¸æ“‡æ¨¡å¼æŸ¥çœ‹è©³ç´°", available_modes, key="detail_metrics_mode")
        results = all_results[selected_mode]
        grouped = calculate_grouped_metrics(results)
        
        # æŒ‰è³‡æ–™ä¾†æº
        st.markdown("### ğŸ“š æŒ‰è³‡æ–™ä¾†æºåˆ†çµ„")
        source_data = []
        for source, stats in grouped["by_source"].items():
            source_data.append({
                "ä¾†æº": source,
                "å•é¡Œæ•¸": stats["total"],
                "Hit Rate": f"{stats['hit_rate']:.2%}" if stats['hit_rate'] else "-",
                "Partial HR": f"{stats['partial_hit_rate']:.2%} ({stats['hit_docs']}/{stats['gold_docs']})",
                "MRR": f"{stats['mrr']:.4f}",
            })
        st.dataframe(pd.DataFrame(source_data), width="stretch", hide_index=True)
        
        # æŒ‰å•é¡Œé¡å‹
        st.markdown("### ğŸ“ˆ æŒ‰å•é¡Œé¡å‹åˆ†çµ„")
        type_data = []
        for q_type, stats in grouped["by_type"].items():
            type_data.append({
                "é¡å‹": q_type,
                "å•é¡Œæ•¸": stats["total"],
                "Hit Rate": f"{stats['hit_rate']:.2%}" if stats['hit_rate'] else "-",
                "Partial HR": f"{stats['partial_hit_rate']:.2%} ({stats['hit_docs']}/{stats['gold_docs']})",
                "MRR": f"{stats['mrr']:.4f}",
            })
        st.dataframe(pd.DataFrame(type_data), width="stretch", hide_index=True)
        
        # ç¸½è¨ˆ
        st.markdown("### ğŸ“Š ç¸½è¨ˆ")
        total = grouped["total"]
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("å•é¡Œæ•¸", total["total"])
        col2.metric("Hit Rate", f"{total['hit_rate']:.2%}" if total['hit_rate'] else "-")
        col3.metric("Partial HR", f"{total['partial_hit_rate']:.2%} ({total['hit_docs']}/{total['gold_docs']})")
        col4.metric("MRR", f"{total['mrr']:.4f}")


def display_results_table(mode: str, results: list[dict]):
    """é¡¯ç¤ºçµæœè¡¨æ ¼"""
    # é è™•ç†æ¬„ä½
    for r in results:
        if "partial_hit" not in r:
            gold_ids = set(r.get("gold_doc_ids", []))
            retrieved_ids = set(r.get("retrieved_doc_ids", []))
            hit_count = len(gold_ids.intersection(retrieved_ids))
            r["partial_hit"] = f"{hit_count}/{len(gold_ids)}"
            r["is_hit"] = hit_count > 0
    
    df = pd.DataFrame([
        {
            "ID": r.get("question_id", "")[:8],
            "å•é¡Œ": (r.get("question", "")[:40] + "...") if len(r.get("question", "")) > 40 else r.get("question", ""),
            "é¡å‹": r.get("question_type", "-"),
            "ä¾†æº": r.get("source_dataset", "-"),
            "å‘½ä¸­": r.get("partial_hit", "-"),
            "Hit": "âœ…" if r.get("is_hit") else "âŒ",
            "LLM": "âœ…" if r.get("is_pass") else ("âŒ" if "is_pass" in r else "-"),
            "Time(ms)": r.get("response_time_ms", 0),
        }
        for r in results
    ])
    
    st.dataframe(df, width="stretch", height=400)


def display_question_detail(results: list[dict], question_idx: int):
    """é¡¯ç¤ºå•é¡Œè©³æƒ…"""
    result = results[question_idx]
    
    # é è™•ç†æ¬„ä½
    gold_ids = set(result.get("gold_doc_ids", []))
    retrieved_ids = result.get("retrieved_doc_ids", [])
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**å•é¡Œ:**")
        st.write(result.get("question", ""))
        
        st.markdown("**æ¨™æº–ç­”æ¡ˆ:**")
        st.info(result.get("gold_answer", "-"))
        
        st.markdown("**æ¨¡å‹å›ç­”:**")
        answer = result.get("generated_answer", "-")
        if result.get("is_pass"):
            st.success(answer)
        elif "is_pass" in result:
            st.error(answer)
        else:
            st.write(answer)
        
        if "llm_judgment" in result:
            st.markdown(f"**LLM åˆ¤æ–·:** {result['llm_judgment']}")
    
    with col2:
        # è¨ˆç®—å‘½ä¸­
        hit_ids = gold_ids.intersection(retrieved_ids)
        partial_hit = f"{len(hit_ids)}/{len(gold_ids)}"
        
        st.markdown("**çµ±è¨ˆ:**")
        st.write(f"- å‘½ä¸­: {partial_hit}")
        st.write(f"- å›æ‡‰æ™‚é–“: {result.get('response_time_ms', 0)} ms")
        
        st.markdown("**Gold Doc IDs:**")
        
        # å¾è³‡æ–™åº«è¼‰å…¥ gold doc å…§å®¹
        from repositories.document_repository import DocumentRepository
        repo = DocumentRepository()
        
        for doc_id in result.get("gold_doc_ids", []):
            is_hit = doc_id in retrieved_ids
            icon = "âœ…" if is_hit else "âŒ"
            
            with st.expander(f"{icon} {doc_id[:20]}..."):
                # å˜—è©¦å¾æª¢ç´¢çµæœä¸­æ‰¾å…§å®¹
                ctx_content = None
                for ctx in result.get("contexts", result.get("retrieved_contexts", [])):
                    if ctx.get("doc_id") == doc_id:
                        ctx_content = ctx.get("content", ctx.get("content_preview"))
                        break
                
                if ctx_content:
                    st.write(ctx_content)
                else:
                    # å¾è³‡æ–™åº«æŸ¥è©¢
                    doc = repo.find_by_doc_id(doc_id)
                    if doc:
                        st.write(doc.get("content", "ç„¡å…§å®¹"))
                    else:
                        st.write("æ‰¾ä¸åˆ°æ­¤æ–‡ä»¶")
    
    # æª¢ç´¢åˆ°çš„æ–‡ä»¶
    contexts = result.get("contexts", result.get("retrieved_contexts", []))
    if contexts:
        st.markdown("### ğŸ“š æª¢ç´¢åˆ°çš„æ–‡ä»¶")
        for i, ctx in enumerate(contexts):
            doc_id = ctx.get("doc_id", "unknown")
            is_gold = doc_id in gold_ids
            icon = "ğŸ¯" if is_gold else "ğŸ“„"
            score = ctx.get("score", 0)
            
            with st.expander(f"{icon} [{i+1}] {doc_id[:16]}... (Score: {score:.4f})"):
                st.markdown(f"**ä¾†æº:** {ctx.get('original_source', '-')}")
                content = ctx.get("content", ctx.get("content_preview", "-"))
                st.write(content)


# ===================== ä¸»ç¨‹å¼ =====================

def main():
    st.title("ğŸ” Hybrid RAG è©•ä¼°å„€è¡¨æ¿")
    
    # è¼‰å…¥å·²å­˜åœ¨çš„çµæœ
    if "results" not in st.session_state:
        st.session_state.results = load_existing_results()
    
    # å´é‚Šæ¬„
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        mode = st.selectbox("æª¢ç´¢æ¨¡å¼", MODES, index=0)
        top_k = st.slider("Top-K", min_value=1, max_value=20, value=5)
        
        st.markdown("---")
        
        if st.button("ğŸš€ åŸ·è¡Œè©•ä¼°", type="primary", use_container_width=True):  # TODO: width param for button
            with st.spinner("è¼‰å…¥æŸ¥è©¢..."):
                queries = load_queries()
            
            st.info(f"æ­£åœ¨ä»¥ {mode} æ¨¡å¼åŸ·è¡Œ {len(queries)} é¡Œ...")
            results = run_evaluation(queries, mode, top_k)
            st.session_state.results[mode] = results
            st.rerun()
        
        if st.button("ğŸ”¬ åŸ·è¡Œ LLM èªæ„è©•ä¼°", use_container_width=True):  # TODO: width param for button
            if mode in st.session_state.results:
                results = run_llm_evaluation(st.session_state.results[mode], mode)
                st.session_state.results[mode] = results
                st.rerun()
            else:
                st.warning("è«‹å…ˆåŸ·è¡Œè©•ä¼°")
        
        if st.button("ğŸ”„ é‡æ–°è¼‰å…¥è³‡æ–™", use_container_width=True):  # TODO: width param for button
            st.session_state.results = load_existing_results()
            st.rerun()
        
        st.markdown("---")
        st.markdown("### å·²æœ‰çµæœ")
        for m in MODES:
            if m in st.session_state.results:
                count = len(st.session_state.results[m])
                has_llm = any("is_pass" in r for r in st.session_state.results[m])
                llm_icon = "ğŸ”¬" if has_llm else ""
                st.write(f"âœ… {m} ({count}é¡Œ) {llm_icon}")
            else:
                st.write(f"â¬œ {m}")
    
    # ä¸»å€åŸŸ
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š æŒ‡æ¨™æ¯”è¼ƒ", "ğŸ“‹ çµæœåˆ—è¡¨", "ğŸ” å•é¡Œè©³æƒ…"])
    
    with tab1:
        display_metrics_comparison(st.session_state.results)
    
    with tab2:
        if st.session_state.results:
            available_modes = [m for m in MODES if m in st.session_state.results]
            selected_mode = st.selectbox("é¸æ“‡æ¨¡å¼", available_modes, key="results_mode")
            display_results_table(selected_mode, st.session_state.results[selected_mode])
        else:
            st.info("è«‹å…ˆåŸ·è¡Œè©•ä¼°æˆ–ç¢ºèª data ç›®éŒ„ä¸­æœ‰çµæœæª”æ¡ˆã€‚")
    
    with tab3:
        if st.session_state.results:
            available_modes = [m for m in MODES if m in st.session_state.results]
            
            col1, col2 = st.columns([1, 3])
            
            with col1:
                selected_mode = st.selectbox("æ¨¡å¼", available_modes, key="detail_mode")
                results = st.session_state.results[selected_mode]
                
                question_options = [
                    f"{i+1}. {r['question'][:25]}..."
                    for i, r in enumerate(results)
                ]
                selected_q = st.selectbox("é¸æ“‡å•é¡Œ", question_options, key="detail_q")
                question_idx = question_options.index(selected_q)
            
            with col2:
                display_question_detail(results, question_idx)
        else:
            st.info("è«‹å…ˆåŸ·è¡Œè©•ä¼°ã€‚")


if __name__ == "__main__":
    main()
