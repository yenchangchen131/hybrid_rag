#!/usr/bin/env python
"""
LLM èªæ„è©•ä¼°è…³æœ¬

ä½¿ç”¨ GPT-4o-mini è©•ä¼°æ¨¡å‹å›ç­”èˆ‡æ¨™æº–ç­”æ¡ˆçš„èªæ„ä¸€è‡´æ€§ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    uv run python scripts/evaluate_answers.py
    uv run python scripts/evaluate_answers.py --input data/rag_results_hybrid.json
"""

import argparse
import json
from pathlib import Path
from datetime import datetime

from tqdm import tqdm
from openai import OpenAI

from core.config import settings


EVALUATION_PROMPT = """è«‹åˆ¤æ–·ã€Œæ¨¡å‹å›ç­”ã€æ˜¯å¦èˆ‡ã€Œæ¨™æº–ç­”æ¡ˆã€èªæ„ä¸€è‡´ã€‚

å•é¡Œï¼š{question}
æ¨™æº–ç­”æ¡ˆï¼š{gold_answer}
æ¨¡å‹å›ç­”ï¼š{model_answer}

åˆ¤æ–·æ¨™æº–ï¼š
- å¦‚æœæ¨¡å‹å›ç­”åŒ…å«æ¨™æº–ç­”æ¡ˆçš„æ ¸å¿ƒè³‡è¨Šï¼Œä¸”æ²’æœ‰æ˜é¡¯éŒ¯èª¤ï¼Œè«‹å›ç­” "Pass"
- å¦‚æœæ¨¡å‹å›ç­”èˆ‡æ¨™æº–ç­”æ¡ˆèªæ„ä¸ä¸€è‡´ã€æœ‰éŒ¯èª¤ã€æˆ–å®Œå…¨ç„¡é—œï¼Œè«‹å›ç­” "Fail"

è«‹åªå›ç­” "Pass" æˆ– "Fail"ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""


def evaluate_answer(
    client: OpenAI,
    question: str,
    gold_answer: str,
    model_answer: str,
    model: str = "gpt-4o-mini",
) -> tuple[str, bool]:
    """ä½¿ç”¨ LLM è©•ä¼°ç­”æ¡ˆ
    
    Returns:
        (raw_response, is_pass)
    """
    prompt = EVALUATION_PROMPT.format(
        question=question,
        gold_answer=gold_answer,
        model_answer=model_answer,
    )
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=10,
        )
        
        raw = response.choices[0].message.content.strip()
        is_pass = raw.lower() == "pass"
        return raw, is_pass
        
    except Exception as e:
        print(f"âš ï¸ è©•ä¼°å¤±æ•—: {e}")
        return f"Error: {e}", False


def evaluate_all(results: list[dict], client: OpenAI) -> list[dict]:
    """è©•ä¼°æ‰€æœ‰çµæœ"""
    evaluated = []
    
    for r in tqdm(results, desc="èªæ„è©•ä¼°ä¸­"):
        raw_response, is_pass = evaluate_answer(
            client=client,
            question=r["question"],
            gold_answer=r.get("gold_answer", ""),
            model_answer=r.get("generated_answer", ""),
        )
        
        evaluated.append({
            "question_id": r["question_id"],
            "question": r["question"],
            "question_type": r.get("question_type", "unknown"),
            "source_dataset": r.get("source_dataset", "unknown"),
            "gold_answer": r.get("gold_answer", ""),
            "generated_answer": r.get("generated_answer", ""),
            "llm_judgment": raw_response,
            "is_pass": is_pass,
        })
    
    return evaluated


def calculate_pass_rate(evaluated: list[dict]) -> dict:
    """è¨ˆç®—é€šéç‡"""
    total = len(evaluated)
    passed = sum(1 for e in evaluated if e["is_pass"])
    
    # æŒ‰å•é¡Œé¡å‹åˆ†çµ„
    by_type = {}
    for e in evaluated:
        q_type = e["question_type"]
        if q_type not in by_type:
            by_type[q_type] = {"total": 0, "passed": 0}
        by_type[q_type]["total"] += 1
        if e["is_pass"]:
            by_type[q_type]["passed"] += 1
    
    # æŒ‰è³‡æ–™ä¾†æºåˆ†çµ„
    by_source = {}
    for e in evaluated:
        source = e["source_dataset"]
        if source not in by_source:
            by_source[source] = {"total": 0, "passed": 0}
        by_source[source]["total"] += 1
        if e["is_pass"]:
            by_source[source]["passed"] += 1
    
    return {
        "summary": {
            "total": total,
            "passed": passed,
            "pass_rate": round(passed / total, 4) if total > 0 else 0,
        },
        "by_question_type": {
            k: {**v, "pass_rate": round(v["passed"] / v["total"], 4) if v["total"] > 0 else 0}
            for k, v in by_type.items()
        },
        "by_source": {
            k: {**v, "pass_rate": round(v["passed"] / v["total"], 4) if v["total"] > 0 else 0}
            for k, v in by_source.items()
        },
    }


def print_results(stats: dict) -> None:
    """è¼¸å‡ºçµæœ"""
    summary = stats["summary"]
    
    print("\n" + "=" * 60)
    print("ğŸ“Š èªæ„è©•ä¼°çµæœ")
    print("=" * 60)
    print(f"ç¸½é¡Œæ•¸:     {summary['total']}")
    print(f"é€šéæ•¸:     {summary['passed']}")
    print(f"Pass Rate:  {summary['pass_rate']:.2%}")
    
    print("\n" + "-" * 60)
    print("ğŸ“ˆ æŒ‰å•é¡Œé¡å‹")
    for q_type, data in stats["by_question_type"].items():
        print(f"  ã€{q_type}ã€‘ {data['pass_rate']:.2%} ({data['passed']}/{data['total']})")
    
    print("\n" + "-" * 60)
    print("ğŸ“š æŒ‰è³‡æ–™ä¾†æº")
    for source, data in stats["by_source"].items():
        print(f"  ã€{source}ã€‘ {data['pass_rate']:.2%} ({data['passed']}/{data['total']})")
    
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description="LLM èªæ„è©•ä¼°")
    parser.add_argument(
        "--input", "-i",
        type=str,
        default="data/rag_results_hybrid.json",
        help="RAG çµæœæª”æ¡ˆè·¯å¾‘"
    )
    parser.add_argument(
        "--output", "-o",
        type=str,
        default=None,
        help="è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ (é è¨­: æ ¹æ“š input è‡ªå‹•å‘½å)"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini",
        help="è©•ä¼°ç”¨ LLM æ¨¡å‹ (é è¨­: gpt-4o-mini)"
    )
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    
    # è‡ªå‹•å‘½åè¼¸å‡ºæª”æ¡ˆ
    if args.output:
        output_path = Path(args.output)
    else:
        input_name = input_path.stem
        mode_suffix = input_name.replace("rag_results_", "")
        output_path = input_path.parent / f"answer_evaluation_{mode_suffix}.json"
    
    print("=" * 60)
    print("ğŸ”¬ LLM èªæ„è©•ä¼°")
    print("=" * 60)
    print(f"ğŸ“‚ è¼¸å…¥: {input_path}")
    print(f"ğŸ¤– æ¨¡å‹: {args.model}")
    
    # è¼‰å…¥çµæœ
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    results = data.get("results", data)
    print(f"   å…± {len(results)} ç­†å¾…è©•ä¼°")
    
    # åˆå§‹åŒ– OpenAI
    client = OpenAI(api_key=settings.OPENAI_API_KEY)
    
    # è©•ä¼°
    evaluated = evaluate_all(results, client)
    
    # è¨ˆç®—çµ±è¨ˆ
    stats = calculate_pass_rate(evaluated)
    
    # è¼¸å‡º
    print_results(stats)
    
    # å„²å­˜
    output_data = {
        "metadata": {
            "input_file": str(input_path),
            "model": args.model,
            "timestamp": datetime.now().isoformat(),
        },
        "statistics": stats,
        "results": evaluated,
    }
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è©³ç´°çµæœå·²å„²å­˜è‡³: {output_path}")


if __name__ == "__main__":
    main()
